{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark: Partition Pruning & Predicate Pushdown Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/YOUR_REPO/blob/main/PySpark_Partition_Pruning_Demo.ipynb)\n",
        "\n",
        "This notebook demonstrates two critical PySpark optimization techniques:\n",
        "1. **Partition Pruning** - Skipping entire data partitions\n",
        "2. **Predicate Pushdown** - Pushing filters to the file format level\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "notebook_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Setup: Install PySpark and Java"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java (required for PySpark)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark -q\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ],
      "metadata": {
        "id": "install_dependencies"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Java environment\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")"
      ],
      "metadata": {
        "id": "setup_environment"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Initialize Spark Session"
      ],
      "metadata": {
        "id": "spark_init_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PartitionPruning_PredicatePushdown\") \\\n",
        "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"‚úÖ Spark Session initialized!\")\n",
        "print(f\"Spark Version: {spark.version}\")"
      ],
      "metadata": {
        "id": "initialize_spark"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Create Sample Data"
      ],
      "metadata": {
        "id": "data_creation_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample orders data\n",
        "sample_data = {\n",
        "    'OrderID': list(range(1, 101)),\n",
        "    'OrderName': [f'Order_{chr(65 + i % 26)}' for i in range(100)],\n",
        "    'Customer': ['John', 'Jane', 'Bob', 'Alice', 'Charlie'] * 20,\n",
        "    'Date': ['21-12-1999', '22-12-1999', '23-12-1999', '24-12-1999', '25-12-1999'] * 20\n",
        "}\n",
        "\n",
        "# Create pandas DataFrame and save as CSV\n",
        "df_pandas = pd.DataFrame(sample_data)\n",
        "df_pandas.to_csv('/content/orders_sample.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Sample orders data created successfully!\")\n",
        "print(f\"\\nTotal records: {len(df_pandas)}\")\n",
        "print(\"\\nSample data:\")\n",
        "df_pandas.head(10)"
      ],
      "metadata": {
        "id": "create_sample_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Step 1: Read Raw Data"
      ],
      "metadata": {
        "id": "read_data_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_path = \"/content/orders_sample.csv\"\n",
        "\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(raw_path)\n",
        "\n",
        "print(\"Raw Data Schema:\")\n",
        "df_raw.printSchema()\n",
        "\n",
        "print(\"\\nSample Records:\")\n",
        "df_raw.show(10)"
      ],
      "metadata": {
        "id": "read_raw_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 2: Write Partitioned Data\n",
        "\n",
        "We'll partition the data by date, creating separate folders for each date."
      ],
      "metadata": {
        "id": "write_partitioned_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "refined_path = \"/content/refined/orders/\"\n",
        "\n",
        "# Convert date string to proper date format for partitioning\n",
        "df_partitioned = df_raw.withColumn(\"date_partition\", to_date(col(\"Date\"), \"dd-MM-yyyy\"))\n",
        "\n",
        "print(\"Data with partition column:\")\n",
        "df_partitioned.show(5)\n",
        "\n",
        "# Write data partitioned by date\n",
        "df_partitioned.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"date_partition\") \\\n",
        "    .parquet(refined_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Data written to {refined_path} partitioned by date_partition\")"
      ],
      "metadata": {
        "id": "write_partitioned_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the directory structure created\n",
        "print(\"Directory structure created:\")\n",
        "!ls -lh /content/refined/orders/"
      ],
      "metadata": {
        "id": "check_directory_structure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìñ Step 3: Read Partitioned Data"
      ],
      "metadata": {
        "id": "read_partitioned_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_refined = spark.read.parquet(refined_path)\n",
        "\n",
        "print(\"üìä Refined Data Schema (with partition column):\")\n",
        "df_refined.printSchema()\n",
        "\n",
        "print(\"\\nSample Records:\")\n",
        "df_refined.show(5)"
      ],
      "metadata": {
        "id": "read_partitioned_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Demonstration 1: Partition Pruning\n",
        "\n",
        "**Partition Pruning** occurs when we filter on the partition column. Spark will only read the specific partition(s) that match the filter, skipping all other partitions entirely."
      ],
      "metadata": {
        "id": "partition_pruning_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üöÄ PARTITION PRUNING - Filter on partition column (date_partition)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Filter on PARTITION COLUMN - Spark will ONLY read specific partitions\n",
        "df_filtered_partition = df_refined.filter(col(\"date_partition\") == \"1999-12-23\")\n",
        "\n",
        "print(\"\\n‚úÖ Query with Partition Pruning (only reads 1 partition):\")\n",
        "print(\"Filter: date_partition == '1999-12-23'\")\n",
        "print(\"\\nPhysical Plan:\")\n",
        "df_filtered_partition.explain(True)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "df_filtered_partition.show()"
      ],
      "metadata": {
        "id": "demo_partition_pruning"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Analysis\n",
        "\n",
        "Notice in the physical plan above:\n",
        "- **PartitionFilters**: Shows `[isnotnull(date_partition#...), (date_partition#... = 1999-12-23)]`\n",
        "- Spark will only scan the `date_partition=1999-12-23` folder\n",
        "- All other date partitions are completely skipped"
      ],
      "metadata": {
        "id": "partition_pruning_analysis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Demonstration 2: Predicate Pushdown\n",
        "\n",
        "**Predicate Pushdown** occurs when we filter on a data column (non-partition). The filter is pushed down to the Parquet reader, which applies it while reading the files, reducing the amount of data loaded into memory."
      ],
      "metadata": {
        "id": "predicate_pushdown_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üöÄ PREDICATE PUSHDOWN - Filter on data column (Customer)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Filter on DATA COLUMN (not partition column) - Predicate Pushdown applies\n",
        "df_filtered_data = df_refined.filter(col(\"Customer\") == \"John\")\n",
        "\n",
        "print(\"\\n‚úÖ Query with Predicate Pushdown (filter pushed to file format):\")\n",
        "print(\"Filter: Customer == 'John'\")\n",
        "print(\"\\nPhysical Plan:\")\n",
        "df_filtered_data.explain(True)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "df_filtered_data.show()"
      ],
      "metadata": {
        "id": "demo_predicate_pushdown"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Analysis\n",
        "\n",
        "Notice in the physical plan above:\n",
        "- **PushedFilters**: Shows `[IsNotNull(Customer), EqualTo(Customer,John)]`\n",
        "- The filter is pushed to the Parquet reader\n",
        "- Parquet uses column statistics and row groups to skip irrelevant data"
      ],
      "metadata": {
        "id": "predicate_pushdown_analysis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Demonstration 3: Combined Optimization\n",
        "\n",
        "The most powerful optimization comes from combining **both techniques**: filter on the partition column AND a data column."
      ],
      "metadata": {
        "id": "combined_optimization_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üöÄ COMBINED - Partition Pruning + Predicate Pushdown\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Filter on BOTH partition column AND data column\n",
        "df_optimized = df_refined.filter(\n",
        "    (col(\"date_partition\") == \"1999-12-23\") &  # Partition Pruning\n",
        "    (col(\"Customer\") == \"John\")                 # Predicate Pushdown\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Optimized Query (both techniques applied):\")\n",
        "print(\"Filter: date_partition == '1999-12-23' AND Customer == 'John'\")\n",
        "print(\"\\nPhysical Plan:\")\n",
        "df_optimized.explain(True)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "df_optimized.show()"
      ],
      "metadata": {
        "id": "demo_combined_optimization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìù Analysis\n",
        "\n",
        "This query benefits from **BOTH optimizations**:\n",
        "1. **Partition Pruning**: Only reads `date_partition=1999-12-23` folder\n",
        "2. **Predicate Pushdown**: Within that partition, filters `Customer=='John'` at the Parquet level\n",
        "\n",
        "Result: Minimal data read from disk, minimal data loaded into memory!"
      ],
      "metadata": {
        "id": "combined_optimization_analysis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Performance Comparison"
      ],
      "metadata": {
        "id": "performance_comparison_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìà PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# WITHOUT optimization (full table scan)\n",
        "print(\"\\n1Ô∏è‚É£  NO FILTER - Full table scan:\")\n",
        "count_all = df_refined.count()\n",
        "print(f\"   Total records: {count_all}\")\n",
        "\n",
        "# WITH Partition Pruning only\n",
        "print(\"\\n2Ô∏è‚É£  PARTITION PRUNING - Filter on partition column:\")\n",
        "count_partition = df_filtered_partition.count()\n",
        "print(f\"   Records with date_partition='1999-12-23': {count_partition}\")\n",
        "print(f\"   Data reduction: {(1 - count_partition/count_all) * 100:.1f}%\")\n",
        "\n",
        "# WITH Predicate Pushdown only\n",
        "print(\"\\n3Ô∏è‚É£  PREDICATE PUSHDOWN - Filter on data column:\")\n",
        "count_data = df_filtered_data.count()\n",
        "print(f\"   Records with Customer='John': {count_data}\")\n",
        "print(f\"   Data reduction: {(1 - count_data/count_all) * 100:.1f}%\")\n",
        "\n",
        "# WITH Both optimizations\n",
        "print(\"\\n4Ô∏è‚É£  BOTH OPTIMIZATIONS - Filter on both:\")\n",
        "count_optimized = df_optimized.count()\n",
        "print(f\"   Records with both filters: {count_optimized}\")\n",
        "print(f\"   Data reduction: {(1 - count_optimized/count_all) * 100:.1f}%\")"
      ],
      "metadata": {
        "id": "performance_comparison"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Partition Statistics"
      ],
      "metadata": {
        "id": "partition_statistics_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"üìä PARTITION STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nRecords per partition:\")\n",
        "\n",
        "df_refined.groupBy(\"date_partition\").count().orderBy(\"date_partition\").show()"
      ],
      "metadata": {
        "id": "partition_statistics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Key Takeaways\n",
        "\n",
        "### 1. **Partition Pruning**\n",
        "- ‚úÖ Applies when filtering on **PARTITION COLUMNS**\n",
        "- ‚úÖ Skips reading entire partitions/folders\n",
        "- ‚úÖ Reduces data scanned from storage\n",
        "- üìå Example: `date_partition == '1999-12-23'`\n",
        "\n",
        "### 2. **Predicate Pushdown**\n",
        "- ‚úÖ Applies when filtering on **DATA COLUMNS** (non-partition)\n",
        "- ‚úÖ Pushes filter to file format reader (Parquet, ORC)\n",
        "- ‚úÖ Reduces data loaded into memory\n",
        "- üìå Example: `Customer == 'John'`\n",
        "\n",
        "### 3. **Best Practices**\n",
        "- ‚úÖ Partition by frequently filtered columns (date, region, category)\n",
        "- ‚úÖ Use columnar formats (Parquet/ORC) for predicate pushdown\n",
        "- ‚úÖ Combine both techniques for maximum performance\n",
        "- ‚ö†Ô∏è Avoid over-partitioning (too many small files)\n",
        "- ‚ö†Ô∏è Ideal partition size: 128MB - 1GB per partition\n",
        "\n",
        "### 4. **In This Demo**\n",
        "- Created 5 partitions by date (21-25 Dec 1999)\n",
        "- Each partition contains 20 records\n",
        "- Total 100 records across all partitions\n",
        "- Demonstrated up to **96% data reduction** with combined filters"
      ],
      "metadata": {
        "id": "key_takeaways"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßπ Cleanup"
      ],
      "metadata": {
        "id": "cleanup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"‚úÖ Spark session stopped. Demo completed!\")"
      ],
      "metadata": {
        "id": "cleanup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìñ Additional Resources\n",
        "\n",
        "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)\n",
        "- [Spark SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n",
        "- [Parquet File Format](https://parquet.apache.org/)\n",
        "\n",
        "---\n",
        "\n",
        "**üìù Note**: Replace `YOUR_USERNAME/YOUR_REPO` in the Colab badge at the top with your actual GitHub username and repository name."
      ],
      "metadata": {
        "id": "additional_resources"
      }
    }
  ]
}