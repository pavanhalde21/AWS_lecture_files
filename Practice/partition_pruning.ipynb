{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4be94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Schema:\n",
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- OrderName: string (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----------+----+----+----+\n",
      "|OrderID|OrderName|Customer|      Date| _c4| _c5| _c6|\n",
      "+-------+---------+--------+----------+----+----+----+\n",
      "|      9|  Order_I|    John|23-12-1999|NULL|NULL|NULL|\n",
      "|     10|  Order_J| Michael|23-12-1999|NULL|NULL|NULL|\n",
      "|     11|  Order_K|    Anna|23-12-1999|NULL|NULL|NULL|\n",
      "|     12|  Order_L|   Chris|23-12-1999|NULL|NULL|NULL|\n",
      "|     13|  Order_M|    John|23-12-1999|NULL|NULL|NULL|\n",
      "|     25|  Order_Y| Michael|23-12-1999|NULL|NULL|NULL|\n",
      "|     30| Order_AD|    John|23-12-1999|NULL|NULL|NULL|\n",
      "|     35| Order_AI| Michael|23-12-1999|NULL|NULL|NULL|\n",
      "|     40| Order_AN|    Mike|23-12-1999|NULL|NULL|NULL|\n",
      "|     45| Order_AS|   Sarah|23-12-1999|NULL|NULL|NULL|\n",
      "+-------+---------+--------+----------+----+----+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PartitionPruning_PredicatePushdown\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: Read raw data from S3\n",
    "# ============================================================\n",
    "raw_path = '/Users/pavanhalde/Downloads/refined/customer/date_partition=1999-12-23/'\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(raw_path)\n",
    "\n",
    "print(\"Raw Data Schema:\")\n",
    "df_raw.printSchema()\n",
    "df_raw.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d767040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Schema:\n",
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- OrderName: string (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- date_partition: date (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "|OrderID|OrderName|Customer|      Date| _c4| _c5| _c6|date_partition|\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "|      9|  Order_I|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     10|  Order_J| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     11|  Order_K|    Anna|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     12|  Order_L|   Chris|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     13|  Order_M|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     25|  Order_Y| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     30| Order_AD|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     35| Order_AI| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     40| Order_AN|    Mike|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     45| Order_AS|   Sarah|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "only showing top 10 rows\n",
      "== Parsed Logical Plan ==\n",
      "'Filter '`=`('date_partition, 1999-12-23)\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "OrderID: int, OrderName: string, Customer: string, Date: string, _c4: string, _c5: string, _c6: string, date_partition: date\n",
      "Filter (date_partition#234 = cast(1999-12-23 as date))\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(date_partition#234) AND (date_partition#234 = 1999-12-23))\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/pavanhalde/Downloads/refined/customer], PartitionFilters: [isnotnull(date_partition#234), (date_partition#234 = 1999-12-23)], PushedFilters: [], ReadSchema: struct<OrderID:int,OrderName:string,Customer:string,Date:string,_c4:string,_c5:string,_c6:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_path = '/Users/pavanhalde/Downloads/refined/customer/'\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(raw_path)\n",
    "\n",
    "\n",
    "df_filtered_partition = df_raw.filter(col(\"date_partition\") == \"1999-12-23\")\n",
    "\n",
    "print(\"Raw Data Schema:\")\n",
    "df_filtered_partition.printSchema()\n",
    "df_filtered_partition.show(10)\n",
    "\n",
    "df_filtered_partition.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b51c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data Schema:\n",
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- OrderName: string (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- date_partition: date (nullable = true)\n",
      "\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "|OrderID|OrderName|Customer|      Date| _c4| _c5| _c6|date_partition|\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "|      9|  Order_I|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     10|  Order_J| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     11|  Order_K|    Anna|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     12|  Order_L|   Chris|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     13|  Order_M|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     25|  Order_Y| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     30| Order_AD|    John|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     35| Order_AI| Michael|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     40| Order_AN|    Mike|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "|     45| Order_AS|   Sarah|23-12-1999|NULL|NULL|NULL|    1999-12-23|\n",
      "+-------+---------+--------+----------+----+----+----+--------------+\n",
      "only showing top 10 rows\n",
      "== Parsed Logical Plan ==\n",
      "'Filter '`=`('date_partition, 1999-12-23)\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "OrderID: int, OrderName: string, Customer: string, Date: string, _c4: string, _c5: string, _c6: string, date_partition: date\n",
      "Filter (date_partition#234 = cast(1999-12-23 as date))\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter (isnotnull(date_partition#234) AND (date_partition#234 = 1999-12-23))\n",
      "+- Relation [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [OrderID#227,OrderName#228,Customer#229,Date#230,_c4#231,_c5#232,_c6#233,date_partition#234] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/pavanhalde/Downloads/refined/customer], PartitionFilters: [isnotnull(date_partition#234), (date_partition#234 = 1999-12-23)], PushedFilters: [], ReadSchema: struct<OrderID:int,OrderName:string,Customer:string,Date:string,_c4:string,_c5:string,_c6:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_path = '/Users/pavanhalde/Downloads/refined/customer/'\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .parquet(raw_path)\n",
    "\n",
    "\n",
    "df_filtered_data = df_raw.filter(col(\"Customer\") == \"John\")\n",
    "\n",
    "print(\"Raw Data Schema:\")\n",
    "df_filtered_partition.printSchema()\n",
    "df_filtered_partition.show(10)\n",
    "\n",
    "df_filtered_partition.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8cef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/15 15:12:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SETUP: Writing data in two formats\n",
      "================================================================================\n",
      "\n",
      "âœ… Written NON-partitioned data to: /tmp/customer_no_partition\n",
      "âœ… Written PARTITIONED data to: /tmp/customer_partitioned\n",
      "\n",
      "================================================================================\n",
      "TEST 1: PREDICATE PUSHDOWN (Non-partitioned data)\n",
      "================================================================================\n",
      "Query: SELECT * WHERE Customer = 'John'\n",
      "\n",
      "ðŸ‘‡ Look for 'PushedFilters:' in the Physical Plan below:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(Customer#302) AND (Customer#302 = John))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [OrderID#300L,OrderName#301,Customer#302,Date#303,date_column#304] Batched: true, DataFilters: [isnotnull(Customer#302), (Customer#302 = John)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/customer_no_partition], PartitionFilters: [], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,John)], ReadSchema: struct<OrderID:bigint,OrderName:string,Customer:string,Date:string,date_column:date>\n",
      "\n",
      "\n",
      "\n",
      "âœ… Results:\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|OrderID|OrderName|Customer|      Date|date_column|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|      1|  Order_A|    John|21-12-1999| 1999-12-21|\n",
      "|      5|  Order_E|    John|22-12-1999| 1999-12-22|\n",
      "|      9|  Order_I|    John|23-12-1999| 1999-12-23|\n",
      "|     13|  Order_M|    John|23-12-1999| 1999-12-23|\n",
      "|     18|  Order_R|    John|25-12-1999| 1999-12-25|\n",
      "|     22|  Order_V|    John|25-12-1999| 1999-12-25|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 2: PARTITION PRUNING (Partitioned data)\n",
      "================================================================================\n",
      "Query: SELECT * WHERE date_column = '1999-12-23'\n",
      "\n",
      "ðŸ‘‡ Look for 'PartitionFilters:' in the Physical Plan below:\n",
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [OrderID#322L,OrderName#323,Customer#324,Date#325,date_column#326] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/customer_partitioned], PartitionFilters: [isnotnull(date_column#326), (date_column#326 = 1999-12-23)], PushedFilters: [], ReadSchema: struct<OrderID:bigint,OrderName:string,Customer:string,Date:string>\n",
      "\n",
      "\n",
      "\n",
      "âœ… Results:\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|OrderID|OrderName|Customer|      Date|date_column|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|     10|  Order_J| Michael|23-12-1999| 1999-12-23|\n",
      "|      9|  Order_I|    John|23-12-1999| 1999-12-23|\n",
      "|     13|  Order_M|    John|23-12-1999| 1999-12-23|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 3: BOTH OPTIMIZATIONS (Partitioned data + data filter)\n",
      "================================================================================\n",
      "Query: SELECT * WHERE date_column = '1999-12-23' AND Customer = 'John'\n",
      "\n",
      "ðŸ‘‡ Look for BOTH 'PartitionFilters:' AND 'PushedFilters:' below:\n",
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(Customer#324) AND (Customer#324 = John))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [OrderID#322L,OrderName#323,Customer#324,Date#325,date_column#326] Batched: true, DataFilters: [isnotnull(Customer#324), (Customer#324 = John)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/customer_partitioned], PartitionFilters: [isnotnull(date_column#326), (date_column#326 = 1999-12-23)], PushedFilters: [IsNotNull(Customer), EqualTo(Customer,John)], ReadSchema: struct<OrderID:bigint,OrderName:string,Customer:string,Date:string>\n",
      "\n",
      "\n",
      "\n",
      "âœ… Results:\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|OrderID|OrderName|Customer|      Date|date_column|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "|      9|  Order_I|    John|23-12-1999| 1999-12-23|\n",
      "|     13|  Order_M|    John|23-12-1999| 1999-12-23|\n",
      "+-------+---------+--------+----------+-----------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ“š SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. PREDICATE PUSHDOWN (Test 1):\n",
      "   âœ“ PushedFilters: [IsNotNull(Customer), EqualTo(Customer,John)]\n",
      "   - Filter pushed to Parquet reader\n",
      "   - Skips rows at file read level\n",
      "\n",
      "2. PARTITION PRUNING (Test 2):\n",
      "   âœ“ PartitionFilters: [isnotnull(date_column#X), (date_column#X = 1999-12-23)]\n",
      "   - Only reads specific partition folder\n",
      "   - Skips entire partitions\n",
      "\n",
      "3. COMBINED (Test 3):\n",
      "   âœ“ PartitionFilters: [date filter]\n",
      "   âœ“ PushedFilters: [Customer filter]\n",
      "   - Best of both worlds!\n",
      "\n",
      "WHY YOU DIDN'T SEE PREDICATE PUSHDOWN:\n",
      "- Your data was already partitioned\n",
      "- Small files without row group statistics\n",
      "- Need larger, non-partitioned files to see it clearly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PredicatePushdownTest\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (1, \"Order_A\", \"John\", \"21-12-1999\"),\n",
    "    (2, \"Order_B\", \"Jane\", \"21-12-1999\"),\n",
    "    (3, \"Order_C\", \"Mike\", \"21-12-1999\"),\n",
    "    (4, \"Order_D\", \"Sarah\", \"21-12-1999\"),\n",
    "    (5, \"Order_E\", \"John\", \"22-12-1999\"),\n",
    "    (9, \"Order_I\", \"John\", \"23-12-1999\"),\n",
    "    (10, \"Order_J\", \"Michael\", \"23-12-1999\"),\n",
    "    (13, \"Order_M\", \"John\", \"23-12-1999\"),\n",
    "    (18, \"Order_R\", \"John\", \"25-12-1999\"),\n",
    "    (22, \"Order_V\", \"John\", \"25-12-1999\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"OrderID\", \"OrderName\", \"Customer\", \"Date\"])\n",
    "df = df.withColumn(\"date_column\", to_date(col(\"Date\"), \"dd-MM-yyyy\"))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SETUP: Writing data in two formats\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================\n",
    "# 1. Write NON-PARTITIONED data (to see predicate pushdown)\n",
    "# ============================================================\n",
    "non_partitioned_path = \"/tmp/customer_no_partition\"\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(non_partitioned_path)\n",
    "print(f\"\\nâœ… Written NON-partitioned data to: {non_partitioned_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. Write PARTITIONED data (to see partition pruning)\n",
    "# ============================================================\n",
    "partitioned_path = \"/tmp/customer_partitioned\"\n",
    "df.write.mode(\"overwrite\").partitionBy(\"date_column\").parquet(partitioned_path)\n",
    "print(f\"âœ… Written PARTITIONED data to: {partitioned_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 1: PREDICATE PUSHDOWN (Non-partitioned data)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Query: SELECT * WHERE Customer = 'John'\")\n",
    "print(\"\\nðŸ‘‡ Look for 'PushedFilters:' in the Physical Plan below:\")\n",
    "\n",
    "df_non_part = spark.read.parquet(non_partitioned_path)\n",
    "df_filtered = df_non_part.filter(col(\"Customer\") == \"John\")\n",
    "\n",
    "df_filtered.explain()\n",
    "print(\"\\nâœ… Results:\")\n",
    "df_filtered.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: PARTITION PRUNING (Partitioned data)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Query: SELECT * WHERE date_column = '1999-12-23'\")\n",
    "print(\"\\nðŸ‘‡ Look for 'PartitionFilters:' in the Physical Plan below:\")\n",
    "\n",
    "df_part = spark.read.parquet(partitioned_path)\n",
    "df_partition_filter = df_part.filter(col(\"date_column\") == \"1999-12-23\")\n",
    "\n",
    "df_partition_filter.explain()\n",
    "print(\"\\nâœ… Results:\")\n",
    "df_partition_filter.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: BOTH OPTIMIZATIONS (Partitioned data + data filter)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Query: SELECT * WHERE date_column = '1999-12-23' AND Customer = 'John'\")\n",
    "print(\"\\nðŸ‘‡ Look for BOTH 'PartitionFilters:' AND 'PushedFilters:' below:\")\n",
    "\n",
    "df_combined = df_part.filter(\n",
    "    (col(\"date_column\") == \"1999-12-23\") & \n",
    "    (col(\"Customer\") == \"John\")\n",
    ")\n",
    "\n",
    "df_combined.explain()\n",
    "print(\"\\nâœ… Results:\")\n",
    "df_combined.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“š SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. PREDICATE PUSHDOWN (Test 1):\n",
    "   âœ“ PushedFilters: [IsNotNull(Customer), EqualTo(Customer,John)]\n",
    "   - Filter pushed to Parquet reader\n",
    "   - Skips rows at file read level\n",
    "   \n",
    "2. PARTITION PRUNING (Test 2):\n",
    "   âœ“ PartitionFilters: [isnotnull(date_column#X), (date_column#X = 1999-12-23)]\n",
    "   - Only reads specific partition folder\n",
    "   - Skips entire partitions\n",
    "   \n",
    "3. COMBINED (Test 3):\n",
    "   âœ“ PartitionFilters: [date filter]\n",
    "   âœ“ PushedFilters: [Customer filter]\n",
    "   - Best of both worlds!\n",
    "   \n",
    "WHY YOU DIDN'T SEE PREDICATE PUSHDOWN:\n",
    "- Your data was already partitioned\n",
    "- Small files without row group statistics\n",
    "- Need larger, non-partitioned files to see it clearly\n",
    "\"\"\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3772527b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067a401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
