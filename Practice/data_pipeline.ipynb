{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4598e4ff",
   "metadata": {},
   "source": [
    "Step 1: Create AWS Session and clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efba428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "aws_session = boto3.Session(\n",
    "    aws_access_key_id='',\n",
    "    aws_secret_access_key='',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "cfg = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"}, read_timeout=60, connect_timeout=10)\n",
    "\n",
    "s3_client = aws_session.client('s3')\n",
    "glue_client = aws_session.client('glue')\n",
    "\n",
    "athena  = aws_session.client(\"athena\", config=cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3b516",
   "metadata": {},
   "source": [
    "Step 2: Extract the data from Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aae12ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Running query: SELECT * FROM olist_brazil_e_commerce.customers\n",
      "/var/folders/19/6l7wlwds1jv0rjylrz6p14yc0000gn/T/ipykernel_56279/2962071395.py:36: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "INFO:root:‚úÖ Uploaded to: s3://golu-aws-project-bucket/raw/customers/customers.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://golu-aws-project-bucket/raw/customers/customers.csv'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import logging\n",
    "\n",
    "def extract_postgres_to_s3(\n",
    "        table_name,\n",
    "        schema=\"public\",\n",
    "        where_clause=None,\n",
    "        partition_date=None,\n",
    "        pg_config=None\n",
    "    ):\n",
    "\n",
    "    # Default configs\n",
    "    pg_config = pg_config or {\n",
    "        'host': 'localhost',\n",
    "        'port': '5432',\n",
    "        'user': 'postgres',\n",
    "        'password': '4518',\n",
    "        'database': 'kaggle_practice'\n",
    "    }\n",
    "\n",
    "    bucket='golu-aws-project-bucket'\n",
    "\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**pg_config)\n",
    "        query = f\"SELECT * FROM {schema}.{table_name}\"\n",
    "        if where_clause:\n",
    "            query += f\" WHERE {where_clause}\"\n",
    "\n",
    "        logging.info(f\"Running query: {query}\")\n",
    "\n",
    "        df = pd.read_sql(query, conn)\n",
    "        if df.empty:\n",
    "            logging.warning(\"No data found.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        # S3 key path\n",
    "        key = f\"raw/{table_name}\"\n",
    "        if partition_date:\n",
    "            key += f\"/dt={partition_date}\"\n",
    "        key += f\"/{table_name}.csv\"\n",
    "\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n",
    "\n",
    "        s3_uri = f\"s3://{bucket}/{key}\"\n",
    "        logging.info(f\"‚úÖ Uploaded to: {s3_uri}\")\n",
    "        return s3_uri\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Failed: {e}\")\n",
    "        raise\n",
    "\n",
    "extract_postgres_to_s3(\"customers\", \"olist_brazil_e_commerce\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "adf43862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.5.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: boto3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.40.64)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pavanhalde/Library/Python/3.13/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.64 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from boto3) (1.40.64)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.64->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pavanhalde/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading mysql_connector_python-9.5.0-cp313-cp313-macosx_14_0_arm64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python pandas boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c6ffd",
   "metadata": {},
   "source": [
    "RDS Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "\n",
    "def extract_rds_mysql_to_s3(\n",
    "        table_name,\n",
    "        database=None,\n",
    "        where_clause=None,\n",
    "        partition_date=None,\n",
    "        rds_config=None,\n",
    "        storage_class='STANDARD_IA'  # Options: STANDARD, INTELLIGENT_TIERING, STANDARD_IA, ONEZONE_IA, GLACIER, DEEP_ARCHIVE\n",
    "    ):\n",
    "\n",
    "    # RDS MySQL configuration\n",
    "    rds_config = rds_config or {\n",
    "        'host': 'brazil-e-commerce-1.cc76oy40c2av.us-east-1.rds.amazonaws.com',  # Your RDS endpoint\n",
    "        'port': 3306,\n",
    "        'user': 'admin',  # Your master username\n",
    "        'password': '',  # Your master password\n",
    "        'database': 'brazil_e_commerce'  # Your database name\n",
    "    }\n",
    "\n",
    "    bucket = 'golu-aws-project-bucket'\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        # Connect to RDS MySQL\n",
    "        logging.info(f\"üîå Connecting to RDS MySQL: {rds_config['host']}\")\n",
    "        conn = mysql.connector.connect(\n",
    "            host=rds_config['host'],\n",
    "            port=rds_config['port'],\n",
    "            user=rds_config['user'],\n",
    "            password=rds_config['password'],\n",
    "            database=rds_config['database'],\n",
    "            connection_timeout=10  # Connection timeout in seconds\n",
    "        )\n",
    "        logging.info(\"‚úÖ Successfully connected to RDS\")\n",
    "\n",
    "        # Build query - MySQL uses backticks for identifiers with special chars\n",
    "        db_prefix = f\"`{database or rds_config['database']}`.\"\n",
    "        query = f\"SELECT * FROM {db_prefix}`{table_name}`\"\n",
    "        if where_clause:\n",
    "            query += f\" WHERE {where_clause}\"\n",
    "\n",
    "        logging.info(f\"üìä Running query: {query}\")\n",
    "\n",
    "        # Execute query and load into DataFrame\n",
    "        df = pd.read_sql(query, conn)\n",
    "        \n",
    "        if df.empty:\n",
    "            logging.warning(\"‚ö†Ô∏è No data found for the given query.\")\n",
    "            return None\n",
    "\n",
    "        logging.info(f\"‚úÖ Extracted {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "        # Build S3 key path\n",
    "        key = f\"raw/RDS/brazil_e_commerce/{table_name}\"\n",
    "        if partition_date:\n",
    "            key += f\"/dt={partition_date}\"\n",
    "        key += f\"/{table_name}.csv\"\n",
    "\n",
    "        # Convert DataFrame to CSV in memory\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        logging.info(f\"üì§ Uploading to S3 with storage class: {storage_class}\")\n",
    "\n",
    "        # Upload to S3 with specified storage class\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            Body=csv_buffer.getvalue(),\n",
    "            StorageClass=storage_class,\n",
    "            ServerSideEncryption='AES256',  # Enable encryption at rest\n",
    "            Metadata={\n",
    "                'source': 'rds-mysql',\n",
    "                'table': table_name,\n",
    "                'database': database or rds_config['database'],\n",
    "                'extracted_rows': str(len(df))\n",
    "            }\n",
    "        )\n",
    "\n",
    "        s3_uri = f\"s3://{bucket}/{key}\"\n",
    "        logging.info(f\"‚úÖ Successfully uploaded to: {s3_uri}\")\n",
    "        logging.info(f\"üì¶ Storage Class: {storage_class}\")\n",
    "        logging.info(f\"üìè File size: {len(csv_buffer.getvalue())} bytes\")\n",
    "        \n",
    "        return s3_uri\n",
    "\n",
    "    except mysql.connector.Error as e:\n",
    "        logging.error(f\"‚ùå Database error: {e}\")\n",
    "        logging.error(\"Check: 1) RDS endpoint, 2) Security group rules, 3) DB credentials\")\n",
    "        raise\n",
    "    \n",
    "    except boto3.exceptions.Boto3Error as e:\n",
    "        logging.error(f\"‚ùå S3 upload failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Unexpected error: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        if conn and conn.is_connected():\n",
    "            conn.close()\n",
    "            logging.info(\"üîå Database connection closed\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USAGE EXAMPLES\n",
    "# ============================================================\n",
    "\n",
    "# Example 1: Extract customers table from your RDS\n",
    "try:\n",
    "    s3_uri = extract_rds_mysql_to_s3(\n",
    "        table_name=\"customers\"\n",
    "    )\n",
    "    print(f\"Data uploaded to: {s3_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"Extraction failed: {e}\")\n",
    "\n",
    "# # Example 2: With date filter and partition (if you have date columns)\n",
    "# try:\n",
    "#     s3_uri = extract_rds_mysql_to_s3(\n",
    "#         table_name=\"customers\",\n",
    "#         where_clause=\"customer_state = 'SP'\",\n",
    "#         partition_date=\"2025-11-08\",\n",
    "#         storage_class='INTELLIGENT_TIERING'  # Auto-optimize storage costs\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"Extraction failed: {e}\")\n",
    "\n",
    "# # Example 3: With custom RDS config\n",
    "# custom_rds_config = {\n",
    "#     'host': 'brazil-e-commerce-1.cc76oy40c2av.us-east-1.rds.amazonaws.com',\n",
    "#     'port': 3306,\n",
    "#     'user': 'admin',\n",
    "#     'password': '',\n",
    "#     'database': 'brazil_e_commerce'\n",
    "# }\n",
    "\n",
    "# try:\n",
    "#     s3_uri = extract_rds_mysql_to_s3(\n",
    "#         table_name=\"customers\",\n",
    "#         rds_config=custom_rds_config,\n",
    "#         storage_class='STANDARD_IA'\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"Extraction failed: {e}\")\n",
    "\n",
    "# # Example 4: Extract multiple tables (when you have more tables)\n",
    "# tables = ['olist_customers_dataset', 'orders', 'order_items', 'products', 'sellers']\n",
    "\n",
    "# for table in tables:\n",
    "#     try:\n",
    "#         logging.info(f\"\\n{'='*60}\")\n",
    "#         logging.info(f\"Processing table: {table}\")\n",
    "#         logging.info(f\"{'='*60}\")\n",
    "        \n",
    "#         s3_uri = extract_rds_mysql_to_s3(\n",
    "#             table_name=table,\n",
    "#             storage_class='STANDARD_IA'\n",
    "#         )\n",
    "#         logging.info(f\"‚úÖ {table} completed successfully\\n\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"‚ùå {table} failed: {e}\\n\")\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8ffae",
   "metadata": {},
   "source": [
    "Step 3: Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd5a5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler 'crawler_07' created. Response HTTPStatusCode: 200\n"
     ]
    }
   ],
   "source": [
    "# Create Glue Crawler\n",
    "\n",
    "crawler_name = \"crawler_07\"\n",
    "role = \"arn:aws:iam::180294202865:role/glue_role_to_give_full_access_to_s3\"\n",
    "database_name = \"mydb_01\"\n",
    "s3_target_path = f\"s3://golu-aws-project-bucket/raw/customers/\"  # adjust path as needed\n",
    "table_prefix = \"raw2_\"\n",
    "\n",
    "\n",
    "try:\n",
    "    response = glue_client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=role,\n",
    "        DatabaseName=database_name,\n",
    "        Description=\"Crawler created via boto3 from Jupyter notebook\",\n",
    "        Targets={\n",
    "            \"S3Targets\": [\n",
    "                {\"Path\": s3_target_path}\n",
    "            ]\n",
    "        },\n",
    "        TablePrefix=table_prefix,\n",
    "        Classifiers=[],\n",
    "        RecrawlPolicy={\"RecrawlBehavior\": \"CRAWL_EVERYTHING\"},\n",
    "        SchemaChangePolicy={\n",
    "            \"UpdateBehavior\": \"UPDATE_IN_DATABASE\",\n",
    "            \"DeleteBehavior\": \"DEPRECATE_IN_DATABASE\"\n",
    "        },\n",
    "        Configuration='{\"Version\":1.0,\"CreatePartitionIndex\":true}'\n",
    "    )\n",
    "    print(f\"Crawler '{crawler_name}' created. Response HTTPStatusCode: {response.get('ResponseMetadata', {}).get('HTTPStatusCode')}\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to create crawler:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e4e754ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler Name: CRAWLER_04, Crawler State: READY\n",
      "Crawler Name: crawler_01, Crawler State: READY\n",
      "Crawler Name: crawler_02, Crawler State: READY\n",
      "Crawler Name: crawler_03, Crawler State: READY\n",
      "Crawler Name: crawler_06, Crawler State: READY\n",
      "Crawler Name: crawler_07, Crawler State: READY\n",
      "Crawler Name: crawler_refined_03, Crawler State: READY\n"
     ]
    }
   ],
   "source": [
    "# List Crawlers ::\n",
    "response = glue_client.get_crawlers()\n",
    "crawlers = response['Crawlers']\n",
    "for crawler in crawlers:\n",
    "    print(f\"Crawler Name: {crawler['Name']}, Crawler State: {crawler['State']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9efe37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '7f42fe9a-bb1e-4190-9663-1dc2bcf9bdb7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 06 Nov 2025 10:29:17 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '7f42fe9a-bb1e-4190-9663-1dc2bcf9bdb7', 'cache-control': 'no-cache'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Start the crawler\n",
    "response = glue_client.start_crawler(Name='crawler_07')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79d259a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Crawler State: READY\n"
     ]
    }
   ],
   "source": [
    "# Get status of the crawler\n",
    "state = glue_client.get_crawler(Name=crawler_name)[\"Crawler\"][\"State\"]\n",
    "print(f\"Current Crawler State: {state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b0c1d7",
   "metadata": {},
   "source": [
    "Step 4: Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "874bf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Glue Job Script to S3 \n",
    "s3_client.upload_file(r\"/Users/pavanhalde/Downloads/glue_job_07.py\", 'golu-aws-project-bucket', 'scripts/glue_job_07.py')\n",
    "\n",
    "# Note: In industry we use CI/CD pipelines to automate such tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8da85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Glue Job \n",
    "response = glue_client.create_job(\n",
    "    Name=\"glue_job_07\",\n",
    "    Role=\"arn:aws:iam::180294202865:role/glue_role_to_give_full_access_to_s3\",\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': \"s3://golu-aws-project-bucket/scripts/glue_job_07.py\"\n",
    "    },\n",
    "    GlueVersion='4.0',\n",
    "    WorkerType='G.1X',\n",
    "    NumberOfWorkers=2,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9dc6dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Name: glue_job_01\n",
      "Job Name: glue_job_02\n",
      "Job Name: glue_job_03\n",
      "Job Name: glue_job_04\n",
      "Job Name: glue_job_05\n",
      "Job Name: glue_job_06\n",
      "Job Name: glue_job_07\n"
     ]
    }
   ],
   "source": [
    "# Listing jobs in account:\n",
    "response = glue_client.get_jobs()\n",
    "jobs=response['Jobs']\n",
    "for job in jobs:\n",
    "    print(f\"Job Name: {job['Name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64a37ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run started with ID: jr_6129933cc70187784ab134707c1109656a01e0e399abb147d0ac8eeb9aebeafb\n"
     ]
    }
   ],
   "source": [
    "# Start Job Run ::\n",
    "response = glue_client.start_job_run(JobName='glue_job_07')\n",
    "job_run_id = response['JobRunId']\n",
    "print(f\"Job run started with ID: {job_run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3bbe5aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Run Status: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# Get Job Run::\n",
    "response = glue_client.get_job_run(JobName='glue_job_07', RunId=job_run_id)\n",
    "job_run = response['JobRun']\n",
    "print(f\"Job Run Status: {job_run['JobRunState']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47152d6f",
   "metadata": {},
   "source": [
    "Step 5: Crawler for refined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13e5bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler 'crawler_refined_04' created. Response HTTPStatusCode: 200\n"
     ]
    }
   ],
   "source": [
    "# Create Glue Crawler\n",
    "\n",
    "crawler_name = \"crawler_refined_04\"\n",
    "role = \"arn:aws:iam::180294202865:role/glue_role_to_give_full_access_to_s3\"\n",
    "database_name = \"mydb_01\"\n",
    "s3_target_path = f\"s3://golu-aws-project-bucket/output/brazil_e_commerce/customer_refined/\" \n",
    "table_prefix = \"refined_\"\n",
    "\n",
    "\n",
    "try:\n",
    "    response = glue_client.create_crawler(\n",
    "        Name=crawler_name,\n",
    "        Role=role,\n",
    "        DatabaseName=database_name,\n",
    "        Description=\"Crawler created via boto3 from Jupyter notebook\",\n",
    "        Targets={\n",
    "            \"S3Targets\": [\n",
    "                {\"Path\": s3_target_path}\n",
    "            ]\n",
    "        },\n",
    "        TablePrefix=table_prefix,\n",
    "        Classifiers=[],\n",
    "        RecrawlPolicy={\"RecrawlBehavior\": \"CRAWL_EVERYTHING\"},\n",
    "        SchemaChangePolicy={\n",
    "            \"UpdateBehavior\": \"UPDATE_IN_DATABASE\",\n",
    "            \"DeleteBehavior\": \"DEPRECATE_IN_DATABASE\"\n",
    "        },\n",
    "        Configuration='{\"Version\":1.0,\"CreatePartitionIndex\":true}'\n",
    "    )\n",
    "    print(f\"Crawler '{crawler_name}' created. Response HTTPStatusCode: {response.get('ResponseMetadata', {}).get('HTTPStatusCode')}\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to create crawler:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac084e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler Name: CRAWLER_04, Crawler State: READY\n",
      "Crawler Name: crawler_01, Crawler State: READY\n",
      "Crawler Name: crawler_02, Crawler State: READY\n",
      "Crawler Name: crawler_03, Crawler State: READY\n",
      "Crawler Name: crawler_06, Crawler State: READY\n",
      "Crawler Name: crawler_07, Crawler State: READY\n",
      "Crawler Name: crawler_refined_03, Crawler State: READY\n",
      "Crawler Name: crawler_refined_04, Crawler State: READY\n"
     ]
    }
   ],
   "source": [
    "# List Crawlers ::\n",
    "response = glue_client.get_crawlers()\n",
    "crawlers = response['Crawlers']\n",
    "for crawler in crawlers:\n",
    "    print(f\"Crawler Name: {crawler['Name']}, Crawler State: {crawler['State']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44a52694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '9d37261e-c734-4df7-93a9-68cb00154164', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 06 Nov 2025 11:00:02 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2', 'connection': 'keep-alive', 'x-amzn-requestid': '9d37261e-c734-4df7-93a9-68cb00154164', 'cache-control': 'no-cache'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Start the crawler\n",
    "response = glue_client.start_crawler(Name='crawler_refined_04')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1aeafd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Crawler State: READY\n"
     ]
    }
   ],
   "source": [
    "# Get status of the crawler\n",
    "state = glue_client.get_crawler(Name=crawler_name)[\"Crawler\"][\"State\"]\n",
    "print(f\"Current Crawler State: {state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bdb0fb",
   "metadata": {},
   "source": [
    "Step 6: Validation through Athena "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5cacb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç VALIDATING GLUE JOB TRANSFORMATIONS\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Configuration\n",
    "WORKGROUP       = \"primary\"\n",
    "OUTPUT_LOCATION = \"s3://golu-aws-project-bucket/athena-results/\"  # For Athena query results only\n",
    "DATABASE        = \"mydb_01\"  # Your Glue database name\n",
    "RAW_TABLE       = \"raw2_customers\"  # Original raw table\n",
    "REFINED_TABLE   = \"refined_customer_refined\"  # Table created by crawler (with prefix)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç VALIDATING GLUE JOB TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def run_athena_query(sql, description):\n",
    "    \"\"\"Helper function to run Athena query and display results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä {description}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Query: {sql[:100]}...\")\n",
    "    \n",
    "    # Start query execution\n",
    "    resp = athena.start_query_execution(\n",
    "        QueryString=sql,\n",
    "        QueryExecutionContext={\"Database\": DATABASE},\n",
    "        WorkGroup=WORKGROUP,\n",
    "        ResultConfiguration={\"OutputLocation\": OUTPUT_LOCATION}\n",
    "    )\n",
    "    qid = resp[\"QueryExecutionId\"]\n",
    "    print(f\"Query ID: {qid}\")\n",
    "    \n",
    "    # Wait for completion\n",
    "    print(\"‚è≥ Waiting for query to complete...\", end=\"\")\n",
    "    while True:\n",
    "        status = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "        if status in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "            break\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    print(f\"\\nStatus: {status}\")\n",
    "    \n",
    "    if status != \"SUCCEEDED\":\n",
    "        detail = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"]\n",
    "        print(f\"‚ùå Query failed: {detail}\")\n",
    "        return None\n",
    "    \n",
    "    # Get results\n",
    "    results = athena.get_query_results(QueryExecutionId=qid)\n",
    "    headers = [c.get(\"VarCharValue\", \"\") for c in results[\"ResultSet\"][\"Rows\"][0][\"Data\"]]\n",
    "    rows = [[c.get(\"VarCharValue\", None) for c in r[\"Data\"]] \n",
    "            for r in results[\"ResultSet\"][\"Rows\"][1:]]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \" | \".join(headers))\n",
    "    print(\"-\" * 60)\n",
    "    for row in rows:\n",
    "        print(\" | \".join(str(cell) if cell else \"NULL\" for cell in row))\n",
    "    print(f\"\\n‚úÖ Rows returned: {len(rows)}\")\n",
    "    \n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "60a59376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Validation 1: Check Uppercase Transformation\n",
      "============================================================\n",
      "Query: \n",
      "SELECT \n",
      "    CASE \n",
      "        WHEN customer_city = UPPER(customer_city) THEN 'PASS - All Uppercase'\n",
      "   ...\n",
      "Query ID: a5dba1a4-b501-4f8e-b90f-0027ab6511b7\n",
      "‚è≥ Waiting for query to complete....\n",
      "Status: SUCCEEDED\n",
      "\n",
      "uppercase_validation | count\n",
      "------------------------------------------------------------\n",
      "PASS - All Uppercase | 41746\n",
      "\n",
      "‚úÖ Rows returned: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['PASS - All Uppercase', '41746']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 1: Check if customer_city is UPPERCASE\n",
    "# ============================================================\n",
    "sql_uppercase_check = f\"\"\"\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN customer_city = UPPER(customer_city) THEN 'PASS - All Uppercase'\n",
    "        ELSE 'FAIL - Not Uppercase'\n",
    "    END AS uppercase_validation,\n",
    "    COUNT(*) AS count\n",
    "FROM {DATABASE}.{REFINED_TABLE}\n",
    "GROUP BY 1;\n",
    "\"\"\"\n",
    "run_athena_query(sql_uppercase_check, \"Validation 1: Check Uppercase Transformation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb5060d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Validation 2: Verify State Filter (Should be SP only)\n",
      "============================================================\n",
      "Query: \n",
      "SELECT \n",
      "    customer_state,\n",
      "    COUNT(*) AS count\n",
      "FROM mydb_01.refined_customer_refined\n",
      "GROUP BY cu...\n",
      "Query ID: 465d56d4-02be-477c-8a9e-b5e7bc97e132\n",
      "‚è≥ Waiting for query to complete....\n",
      "Status: SUCCEEDED\n",
      "\n",
      "customer_state | count\n",
      "------------------------------------------------------------\n",
      "SP | 41746\n",
      "\n",
      "‚úÖ Rows returned: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['SP', '41746']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 2: Verify only SP state exists\n",
    "# ============================================================\n",
    "sql_state_check = f\"\"\"\n",
    "SELECT \n",
    "    customer_state,\n",
    "    COUNT(*) AS count\n",
    "FROM {DATABASE}.{REFINED_TABLE}\n",
    "GROUP BY customer_state\n",
    "ORDER BY count DESC;\n",
    "\"\"\"\n",
    "run_athena_query(sql_state_check, \"Validation 2: Verify State Filter (Should be SP only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "411b387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Validation 3: Check Dropped Columns\n",
      "============================================================\n",
      "Query: Fetching table schema...\n",
      "\n",
      "Columns in refined table:\n",
      "  1. customer_id\n",
      "  2. customer_zip_code_prefix\n",
      "  3. customer_city\n",
      "  4. customer_state\n",
      "\n",
      "‚úÖ PASS: customer_unique_id column was successfully dropped!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 3: Check if customer_unique_id column was dropped\n",
    "# ============================================================\n",
    "sql_column_check = f\"\"\"\n",
    "SELECT * \n",
    "FROM {DATABASE}.{REFINED_TABLE} \n",
    "LIMIT 1;\n",
    "\"\"\"\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä Validation 3: Check Dropped Columns\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"Query: Fetching table schema...\")\n",
    "\n",
    "resp = athena.start_query_execution(\n",
    "    QueryString=sql_column_check,\n",
    "    QueryExecutionContext={\"Database\": DATABASE},\n",
    "    WorkGroup=WORKGROUP,\n",
    "    ResultConfiguration={\"OutputLocation\": OUTPUT_LOCATION}\n",
    ")\n",
    "qid = resp[\"QueryExecutionId\"]\n",
    "\n",
    "# Wait for completion\n",
    "while True:\n",
    "    status = athena.get_query_execution(QueryExecutionId=qid)[\"QueryExecution\"][\"Status\"][\"State\"]\n",
    "    if status in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
    "        break\n",
    "    time.sleep(2)\n",
    "\n",
    "if status == \"SUCCEEDED\":\n",
    "    results = athena.get_query_results(QueryExecutionId=qid)\n",
    "    headers = [c.get(\"VarCharValue\", \"\") for c in results[\"ResultSet\"][\"Rows\"][0][\"Data\"]]\n",
    "    print(\"\\nColumns in refined table:\")\n",
    "    for i, col in enumerate(headers, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "    if \"customer_unique_id\" in headers:\n",
    "        print(\"\\n‚ùå FAIL: customer_unique_id column still exists!\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ PASS: customer_unique_id column was successfully dropped!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "640f7b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Validation 4: Compare Raw vs Refined Row Counts\n",
      "============================================================\n",
      "Query: \n",
      "SELECT \n",
      "    'Raw Table' AS source,\n",
      "    COUNT(*) AS row_count\n",
      "FROM mydb_01.raw_customers\n",
      "UNION ALL\n",
      "S...\n",
      "Query ID: 520f4847-135e-4832-b2bd-9d68ca3f868a\n",
      "‚è≥ Waiting for query to complete....\n",
      "Status: SUCCEEDED\n",
      "\n",
      "source | row_count\n",
      "------------------------------------------------------------\n",
      "Refined Table (SP only) | 41746\n",
      "Raw Table | 99441\n",
      "\n",
      "‚úÖ Rows returned: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Refined Table (SP only)', '41746'], ['Raw Table', '99441']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 4: Compare Raw vs Refined Row Counts\n",
    "# ============================================================\n",
    "sql_compare_counts = f\"\"\"\n",
    "SELECT \n",
    "    'Raw Table' AS source,\n",
    "    COUNT(*) AS row_count\n",
    "FROM {DATABASE}.{RAW_TABLE}\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Refined Table (SP only)' AS source,\n",
    "    COUNT(*) AS row_count\n",
    "FROM {DATABASE}.{REFINED_TABLE};\n",
    "\"\"\"\n",
    "run_athena_query(sql_compare_counts, \"Validation 4: Compare Raw vs Refined Row Counts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4bad3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä Validation 5: Sample Refined Data (First 10 rows)\n",
      "============================================================\n",
      "Query: \n",
      "SELECT \n",
      "    customer_id,\n",
      "    customer_zip_code_prefix,\n",
      "    customer_city,\n",
      "    customer_state\n",
      "FROM m...\n",
      "Query ID: 532e971a-80df-4e7e-8e01-56cc20ed80d4\n",
      "‚è≥ Waiting for query to complete....\n",
      "Status: SUCCEEDED\n",
      "\n",
      "customer_id | customer_zip_code_prefix | customer_city | customer_state\n",
      "------------------------------------------------------------\n",
      "06b8999e2fba1a1fbc88172c00ba8bc7 | 14409.0 | FRANCA | SP\n",
      "18955e83d337fd6b2def6b18a428ac77 | 9790.0 | SAO BERNARDO DO CAMPO | SP\n",
      "4e7b3e00288586ebd08712fdd0374a03 | 1151.0 | SAO PAULO | SP\n",
      "b2b6027bc5c5109e529d4dc6358b12c3 | 8775.0 | MOGI DAS CRUZES | SP\n",
      "4f2d8ab171c80ec8364f7c12e35b23ad | 13056.0 | CAMPINAS | SP\n",
      "fd826e7cf63160e536e0908c76c3f441 | 4534.0 | SAO PAULO | SP\n",
      "b2d1536598b73a9abd18e0d75d92f0a3 | 18682.0 | LENCOIS PAULISTA | SP\n",
      "eabebad39a88bb6f5b52376faec28612 | 5704.0 | SAO PAULO | SP\n",
      "206f3129c0e4d7d0b9550426023f0a08 | 13412.0 | PIRACICABA | SP\n",
      "c5c61596a3b6bd0cee5766992c48a9a1 | 7124.0 | GUARULHOS | SP\n",
      "\n",
      "‚úÖ Rows returned: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['06b8999e2fba1a1fbc88172c00ba8bc7', '14409.0', 'FRANCA', 'SP'],\n",
       " ['18955e83d337fd6b2def6b18a428ac77', '9790.0', 'SAO BERNARDO DO CAMPO', 'SP'],\n",
       " ['4e7b3e00288586ebd08712fdd0374a03', '1151.0', 'SAO PAULO', 'SP'],\n",
       " ['b2b6027bc5c5109e529d4dc6358b12c3', '8775.0', 'MOGI DAS CRUZES', 'SP'],\n",
       " ['4f2d8ab171c80ec8364f7c12e35b23ad', '13056.0', 'CAMPINAS', 'SP'],\n",
       " ['fd826e7cf63160e536e0908c76c3f441', '4534.0', 'SAO PAULO', 'SP'],\n",
       " ['b2d1536598b73a9abd18e0d75d92f0a3', '18682.0', 'LENCOIS PAULISTA', 'SP'],\n",
       " ['eabebad39a88bb6f5b52376faec28612', '5704.0', 'SAO PAULO', 'SP'],\n",
       " ['206f3129c0e4d7d0b9550426023f0a08', '13412.0', 'PIRACICABA', 'SP'],\n",
       " ['c5c61596a3b6bd0cee5766992c48a9a1', '7124.0', 'GUARULHOS', 'SP']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 5: Sample refined data\n",
    "# ============================================================\n",
    "sql_sample = f\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    customer_zip_code_prefix,\n",
    "    customer_city,\n",
    "    customer_state\n",
    "FROM {DATABASE}.{REFINED_TABLE}\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "run_athena_query(sql_sample, \"Validation 5: Sample Refined Data (First 10 rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a169aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# VALIDATION 6: Top cities in refined data\n",
    "# ============================================================\n",
    "sql_top_cities = f\"\"\"\n",
    "SELECT \n",
    "    customer_city,\n",
    "    customer_state,\n",
    "    COUNT(*) AS customer_count\n",
    "FROM {DATABASE}.{REFINED_TABLE}\n",
    "GROUP BY customer_city, customer_state\n",
    "ORDER BY customer_count DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "run_athena_query(sql_top_cities, \"Validation 6: Top 10 Cities in Refined Data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ ALL VALIDATIONS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSummary of Validations:\")\n",
    "print(\"1. ‚úì Uppercase transformation on customer_city\")\n",
    "print(\"2. ‚úì State filter (SP only)\")\n",
    "print(\"3. ‚úì Column drop (customer_unique_id)\")\n",
    "print(\"4. ‚úì Row count comparison\")\n",
    "print(\"5. ‚úì Sample data inspection\")\n",
    "print(\"6. ‚úì Top cities analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704676a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703e16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4a68b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110804e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ade3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553262e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
